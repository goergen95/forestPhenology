---
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
    latex_engine: pdflatex
    fig_caption: true
    keep_tex: true
header-includes:
  \usepackage{float}
bibliography: ["ForestPhenology.bib", "r-bib.bib"] 
csl: elsevier-harvard.csl
nocite: |
@Broge2001, @Gobron2000, @Wan2018, @Rowan2003, @Laboratory2015, @Sonnentag2012, @Richardson2009, @Hunt2012
title: "UAV imagery based tree species classification in the Marburg OpenForest"
author: 
- Darius A. Görgen
- Tobias Koch
- Marvin Müsgen
- Eike Schott
date: "`r format(Sys.time(), '%B %d, %Y')`"

abstract: "The monitoring of forests environments is of crucial importance since they serve as natural habitats and constitute a main source of biological diversity on the planet. Yet, it is very costly and labor intensive to monitor forests by traditional means and classical remote sensing technologies restrict the analysis to the regional level. To overcome these challenges there have been several attempts to use UAV-borne imagery in forest monitoring. By the use of drones images can be obtained at low cost and can be associated with both, high spatial and temporal resolution. This enables scientists and practitioners to comprehensively monitor forest environments. In this paper we present our results of an experiment exploring the influence of spatial resolution of RGB imagery, artificially derived vegetation indices as well as seasonal parameters on the accuracy of tree species classification within the Marburg OpenForest. We used a resolution of 10, 15, and 25 cm in a forward-feature-selection based on the Random Forest classification algorithm. Additionally we tested the obtained accuracy when only mono-temporal or multi-temporal variables are included as well as both types of variables. Our results show that accuracy is prone to errors in pixel georeferencing and tree location. Object-based classification methods might lead to higher classification accuracies, but the construction of balanced training dataset is of preliminary importance and needs further improvement for our study area."

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", fig.align="center")
library(tmap)
library(caret)
library(raster)
library(stringr)
library(ggplot2)
library(dplyr)
library(purrr)
library(rgdal)
library(knitr)
library(kableExtra)
library(tidyr)
library(png)
library(forcats)
library(randomForest)
library(gridExtra)
trees = readOGR("../../data/trees_buffer.shp")
```

\newpage

# Introduction

Forests environments provide valuable services to the human well being as well as 
supporting services to the function of ecosystems. Additionally, they count to 
the main biodiversity hotspots on earth [@Brockerhoff2017]. However, the most recent
Global Forest Resource Assessment of the FAO states, that the global forest area
shrinked between 1990 and 2015 from 31.6% of the global land cover to 30.6%, or in 
other numbers from 4,128 Mio. ha to 3,999 Mio. ha - a decrease of 3.1% [@FoodandAgricultureOrgansiationoftheUnitedNations2015].
Simultaneously, the percentage of planted trees increased by over 105 Mio. ha, 
reducing the share of natural forest areas. Over the last decades, we can find growing
scholar interests on ecosystem services and functions which are sustained by forests,
mainly the habitat provision for endangered or native species, the provision of 
material goods such as wood biomass, soil formation and composition, as well as 
climatic regulation functions, such as carbon sequestration [@Brockerhoff2017].
One of the main critical variables in assessing the quality of forest environments,
their biodiversity, and their structural attributes is the tree species, either on
the individual tree level or the dominating species for coarser areas of interest. 

Identifying tree species, however, on an operative scale for larger areas, remains a challenge.
Recently, different remote sensing approaches proved that tree species identification
from above the earth's surface is feasible, but there still remain significant 
trade-offs between the costs, computational demand and spatial-temporal resolution of
remotely sensed imagery. Some studies have used satellite imagery which most frequently also
include information in the infrared spectrum [@Zhang2003; @Elatawneh2013; @Ulsig2017a; @Persson2018; @Grabska2019]
but generally shows a relatively low spatial resolution limiting the analysis to a 
level of stand rather than individuals. Additionally, depended on the platform's orbit
and current weather conditions, the temporal resolution may vary significantly and is 
not completely planable.

One technology to partly overcome these restrictions is the use of Radio Detection And
Ranging (RADAR) sensors which show a lower dependency of data quality to the presence
of clouds and fog. Recently, multi-temporal data from Sentinel-1 has been used in 
conjunction with spectral data to not only improve the species classification accuracy but
also to retrieve additional parameters important to forest monitoring such as forest type, 
stand density, annual phenology, and biomass production among others [@Frison2018; @Niculescu2018; @Dostalova2018; @Mngadi2019].
However promising these advances, the analysis are most commonly restricted to regional analysis
of forest structures. On a more localized level, high resolution satellite data is 
either not available or associated with very high costs. With the rapid development of
unmanned aerial vehicles (UAV) during the last years and a significant decrease in price for
this technology, new approaches to monitor forest structures on a very local level
have recently emerged [@Yao2019b].  

UAVs serve as an aerial platform of different kind of sensors which can range from
LidAR [@Fricker2019], hyper- and multi spectral sensors [@Marques2019a; @Berra2019] 
as well as simple RGB cameras [@Natesan2019]. A broad methodology to obtain species information for 
single tree individuals has been developed integrating the calculation of various 
vegetation indices from mono- and multi-temporal imagery and the use of machine learning
models to derive a relationship between the measured variables and the 
tree species.
@Berra2016 used the Green Chromatic Coordinate to monitor the Start-of-Season for
four different tree species in deciduous woodland suggesting that UAV imagery 
can contribute to investigate the phenological status of individual trees.
@Klosterman2017 were able to estimate phenological status on the leaf-level
based on the calculation of the green and red chromatic coordinate (GCC and RCC) 
during spring and autumn to model bud burst and leaf expansion as well as leaf senescence.
@Natesan2019 used Residual Neural Networks to classify three different tree species based on RGB
imagery obtained over the course of three years and achieved a classification accuracy
of about 80%, and an accuracy of 51% when only the data of single years was used. 
@Fricker2019 used hyperspectral images obtained by a UAV and a Convolutional Neural
Network to classify tree species. They also underwent an experiment which only included RGB
data. The hyperspectral data achieved an F-score of 0.87, while the RGB data achieved 
a score of 0.64 in a dominantly coniferous forest.

Additionally, the development of structure-from-motion algorithms 
to get 3D information from 2D RGB imagery taken from slightly different angles 
have enriched the analysis of forest structures from low-cost sensors. 
@Nevalainen2017 used RGB images and an automated matching technique to obtain 
point clouds at a 5cm resolution. Coupled with hyperspectral imagery this allowed 
the tree species classification to an accuracy at 95%.
@Yan2018a compared their approach of retrieving tree crowns from RGB images with
crowns delineated from LiDAR data and report an accuracy at about 90%. @Krause2019a 
were able to retrieve individual tree height based on a photogrammetric point-cloud 
with an RMSE at about 2-3 %. @Brieger2019a used RGB derived point clouds at
different study sites in Siberia and achieved an accuracy of 67.1% in delineating 
individual tree crowns and an RMSE of 18.46% for tree height. @Sothe2019a used
hyperspectral images for tree delineation and classification in subtropical rain forests
and achieved and Kappa score of 0.7 (overall accuracy of 72.4%) by combining spectral
raw data, indices as well as structural parameters in the classification process using 
a support-vector machine. @Richardson2009 proved, that simple RGB images can 
contribute to seasonally differentiate tree green-up and senescence
in different forest environments.

However, little efforts have been done to structurally investigate the impact of decreasing
spatial resolution on the classification accuracy as well as the impact of
the integration of seasonal parameters derived from multiple mono-temporal observations.
Here, we strictly limit our analysis to the investigation of these two thematic 
blocs and deliberately exclude other factors such as structural variables obtained
from point clouds. We solely focus on the analysis of the dynamic in predictive potential 
of RGB derived mono- and multi-temporal variables to model tree species with 
changing spatial resolution. 

To this end we use the RGB imagery obtained by multiple overflights during the year 
2019 from a study side located within the Marburg University forest which is part
of the research project [Natur 4.0](https://www.uni-marburg.de/en/fb19/natur40/). 
This forest is used as a joint research area for a project between several German universities 
and research institutes and sets out to investigate the potential of sensor technology for 
biodiversity and natural resource management in natural forest environments. 
We artificially decreased the spatial resolution of aerial imagery obtained in this
area resulting in three different target resolutions of 10, 15, and 25 cm. 
For every single overflight we calculated a series of RGB indices on a pixels basis. 
Additionally, we calculated descriptive statistics (mean, maximum, minimum, amplitude, etc.) 
for each index over the course of the year to include information about the 
seasonality of the phenological development.

The resulting input data is used to establish a total of nine distinct random forest models, one
for each resolution including either only mono-temporal or seasonal predictor variables 
or both types of variables. On the basis of a forward-feature selection
the variables which carry the most relevant information content for the tree species
classification are then selected and used in the species prediction. The evaluation
of the classification accuracies is compared on a pixel and object basis to draw
conclusions on the importance of spatial resolution as well as the importance
of mono- vs. multi-temporal input data.


# Data and Methods

## Study Area

The study area is located at 50.8°N 8.7°E in the German low mountain range in Hessen.
It is part of the University Forest Caldern where the recently initiated joint research
project Natur 4.0 aims at investigating the use of networked sensor technology for 
biodiversity and ecosystem management and protection. It is in this context, that the aerial images 
which we used here were obtained in an observation campaign during 2019 (see Tab. 1).
The area is approximately 37,500 m² and continuously covered by trees. In this
specific location, only two distinct species are present with stem diameters
at breast height (DBH) greater than 40 cm, namely _fagus sylvatica_ and
_quercus robur_.


```{r plot_aoi, warning=F, message=F,fig.cap="RGB image of the study area from May 16th 2019 with central positions of tree stems superimposed (Coordinates are presented in UTM32N).", out.width="60%", fig.pos="H"}

rgb = brick("../../data/resampled/res25.tif")[[10:12]]
projection = "+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"

tm_shape(rgb)+
  tm_rgb()+
  tm_shape(trees)+
  tm_borders(lwd=1, lty="solid", col="black")+
  tm_graticules(ticks=T, projection=projection, lines=F, 
                labels.format=list(digits=0),
                n.y=6, n.x=4)+
  tm_layout(inner.margins = 0)
```

## Pre-Processing of the UAV orthoimages


In this project, AgisoftPhotoScan was used to process the UAV imagery. Agisoft 
Photoscan Professional is an affordable 3D reconstruction software from the 
Russian company Agisoft LLC @Agisoft2019 for the generation of dense point clouds 
and photogrammetric products such as orthorectified mosaics and digital surface models (DSM) derived from point clouds. Photoscan has the advantage to provide a simple workflow, from performing 
bundle block adjustment to calibrate the camera and orientate images after automatic 
tie point measurements, geo-referencing by measuring ground control points, 
concluding with the computation of a dense point cloud and requested final 
products [@Mayer2018]. An airborne system was used to acquire the imagery 
using a commercial GoPro in several flight campaigns with a flight altitude of 
40 meters The internal GPS of the GoPro was used for geotagging the images. A 
post referencing enabled a better processing of the images in SFM software and 
more accurate orthophotos without manual referencing in Photoscan. Photo 
Alignment is a process in PS for image matching and bundle block adjustment in 
an arbitrary system. It generates a sparse point cloud as well as the interior 
and exterior orientation parameters of all images in that system, including 
systematic error compensation such as non-linear lens distortions. Prior to 
the adjustment, the tie points are automatically measured by detecting and 
matching features in overlapping images resulting in a sparse point cloud 
[@Mayer2018]. 
The settings in this project were chosen as follows:

- General: Accuracy: Medium; Generic preselection: yes; Reference preselection: yes

- Advanced: Key Point limit: 40000; Tie point limit: 4000; apply mask: no; Adaptive camera model fitting: yes

Sparse cloud filtering was performed under the following settings:

- gradual selection: reprojection error: 0.26; reconstruction uncertainty: 189.461; 
projection accuracy: 12.4621; reconstruction uncertainty: 6.72951; reprojection error: 0.122199

```{r dates-table, tab.cap="Days of UAV overpass."}
dates = unique(readRDS("../../data/resampled/dates.rds"))
dates = as.Date(dates, format = "%Y_%m_%d" )
dates = data.frame(Dates=dates)
knitr::kable(dates, format = "latex", linesep="", align = "l", booktabs = T,
             caption = "Dates of the UAV overpasses.") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  row_spec(0,bold=TRUE)
```

Based on the information of the point cloud PhotoScan can construct a polygon model 
(e.g. mesh) [@Agisoft2019]. In this project the mesh was build by the following settings:

- General: surface type: Height field (2.5D); source data: sparse cloud; face count: medium

- Advanced: interpolation: enabled; point classes: all; calculate vertex colors: no

The different pixel values from different photos are combined by the mosaic type 
in the final texture. Mosaic type implies a two-step approach. Low frequency 
components are blended for overlapping images to avoid seam line problems. 
The high-frequency component, on the other hand, which is responsible for the 
image details, is only captured from a single image.

In total we worked on 6 images which were obtained between the end of April to 
the end of June (see Table 1). In the mid latitudes of central Europe these are the 
months of vegetational peak of mixed forests.
However, it must be noted, that the images provided by the Nature 4.0 project 
showed a rather large margin of error when it comes to the accuracy of georeferencing 
Therefore, a number of locations in the images are not optimally overlaid, 
which leads to image distortions. The following figure shows the image distortion 
for a sample location. This location is not included in our study area, where we suggest
the difference of localization to be much smaller than in the image below. However,
we could not exclude the influence of these distortions completely. 

```{r Image distortion, echo=F, message=F, warning=F, fig.cap="Display of the image distortion in the data basis (The car as well as the rectangle form on the right of the image are visible multiple times at slightly different locations due to the construction of the mesh).", out.width="60%"}
imgr <- stack("Bild_verzerrung.tif")
plotRGB(imgr)

```


## Tree species data

With the use of a differential GPS the position of tree stems within the study area
was logged during a field campaign. Associated with the positional data, the
tree species as well as the DBH was collected. As stated before, here we only focused
on the determination of the impact of changing resolutions and multi-temporal
predictor variables on the classification accuracy. Therefore, we simplified the
delineation of tree crowns corresponding to the needs of the investigation. First,
we excluded all trees with a DBH below 40 cm, because we assumed that the crowns
of greater trees would cover the smaller ones and thus they could not be observed
on aerial images from above the crown surface. Secondly, we buffered the central 
positions of the residual trees by a square of 2 x 2 m, assuming that with this
size we would essentially cover substantial proportions of the associated tree 
crown (Fig. 1). However, some of these buffered polygons intersected. 
In these cases, we decided to exclude both intersecting polygons since any 
decision to keep one over the other would be arbitrary. In the end, we obtained 
`r nrow(trees@data)` tree individuals of which `r length(which(trees$specID == "BUR"))` 
(`r round(length(which(trees$specID == "BUR")) / nrow(trees@data) * 100)`%)  were _fagus sylvatica_ 
and `r length(which(trees$specID == "EIT"))` 
(`r round(length(which(trees$specID == "EIT")) / nrow(trees@data) * 100)`%) were _quercus robur_.

## RGB indices

We calculated a number of seven color vegetation indices (VI) which can be found in Table 2. 
These color indices were suspected to care information content on the phenological 
development of tree species during a year. Indices are frequently used in remote sensing
studies due to their relational nature which compensates for influences of illumination and viewing 
geometry on the measured reflectance values of the RGB channels. The chosen indices
delivered satisfying results in prior studies, which corresponding sources 
are reported in Table 2.
They were calculated for each UAV overpass resulting in (7VIs+R+G+B) x 6 observations = 60 mono-temporal predictor
variables. Additionally, we calculated the maximum (MAX), minimum (MIN), sum (SUM), standard
deviation (SD), amplitude (AMP) as well as the 25%- (Q25) and 75%-percentile (Q75) values
for each VI and the RGB channels and the raw channels across the time series resulting in additional 70 seasonal predictors. 

```{r indices, echo=F, message=F, warning=F}
indicesdf = data.frame(name = c("Triangular greenness index",
                                "Green Leaf Index",
                                "Color Index of Vegetation",
                                "Iron Oxide Index",
                                "Visible Vegetation Index",
                                "Green Chromatic Coordinate",
                                "Red Chromatic Coordinate"),
                       abbreviation = c("TGI", 
                                        "GLI", 
                                        "CIVE", 
                                        "IO", 
                                        "VVI", 
                                        "GCC",
                                        "RCC"),
                       formula = c("-0.5 * (190*(R-G) - 120*(R-B))",
                                   "(2*G-R-B) / (2*G+R+B)",
                                   "(0.441*R-0.881*G + 0.385*B + 18.787)",
                                   "R/B",
                                   "(1-|R-30| / |R+30|) * \n
                                 (1- |G-50| / |G+50|) * \n 
                                 (1-|B-1| / |B+1|)",
                                 "G / (R+G+B)",
                                 "R / (R+G+B)"),
                       reference = c("Broge and Leblanc (2001)",
                                     "Gobron et al. (2000)",
                                     "Wan et al. (2018)",
                                     "Rowan and Mars (2003)",
                                     "Planetary Habitability Laboratory (2015)",
                                     "Sonnentag et al. (2012)",
                                     "Richardson et al. (2009)"))

kable(indicesdf, format = "latex", linesep="", align = "l", booktabs = T,
      caption = "Names and formulas of RGB indices.") %>%
  kable_styling(latex_options = c("scale_down","HOLD_position")) %>%
  row_spec(0,bold=TRUE) %>%
  footnote(general = "R: 580-670 nm, G: 480-610 nm, B: 400-520 nm, for digital cameras according to Hunt et al. (2012).")
```

Fig. 3 shows the trajectory of the calculated VIs according to the tree species
indicating the mean value (points) as well as +/- one standard deviation from the mean
(vertical bars) based on all pixels within the respective tree polygons per class. The trajectories are very
similar for both classes, however, for almost all VIs there are substantial differences
between the classes during the first few observations from April to May. We also
see, that bettwen the 16th of May and the 5th of June, we have a relatively large
period with no observations.

```{r predictor_plots_indices, echo=F, message=F, warning=F, fig.cap="Temporal dynamic of calculated VIs over the course of the year by tree species at 25 cm resolution (points represent the average value, the bars represent +/- one standard deviation)."}

if (!file.exists("ind_data.rds")){
  indices_names = readRDS("../../data/indices/names_indices_stack.rds")
  indices = raster::stack(list.files("../../data/indices", pattern="res10", full.names=TRUE))
  names(indices) = indices_names
  fagus =  rasterize(trees[trees$specID == "BUR",], rgb, 1)
  quercus =  rasterize(trees[trees$specID == "EIT",], rgb, 1)
}


if (!file.exists("ind_data.rds")){
  ind_fagus = as_tibble(indices[fagus])
  ind_fagus$species = "fagus"
  
  ind_quercus = as_tibble(indices[quercus])
  ind_quercus$species = "quercus"
  ind = bind_rows(ind_fagus, ind_quercus)
  substr(names(ind)[1:42], 12, 12) = "-"
  
  ind = ind %>%
    gather(key, value, -species) %>%
    separate(key, c("date", "index"), "-") %>%
    mutate(date = as.Date(date, format = "X%Y_%m_%d"))
  saveRDS(ind, "ind_data.rds")
} else {
  ind = readRDS("ind_data.rds")
}

max_sd = function(x) { mean(x) + sd(x)}
min_sd = function(x) { mean(x) - sd(x)}

ggplot(ind, aes(x=date, y=value, col=species)) +
  stat_summary(fun.y = mean, 
               fun.ymin = min_sd,
               fun.ymax = max_sd,
               geom = "pointrange",
               alpha = .5) +
  stat_summary(fun.y = mean,
               geom = "line",
               alpha = .7) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_x_date(date_labels = "%b %d", breaks  = unique(ind$date))+
  facet_wrap(. ~ index, scales = "free")+
  ylab("Value of corresponding index")+
  xlab("Date")

```

Concerning the frequency of the seasonal predictor variables both tree species
are characterized by a very similar distribution (Fig. 4), however, we see that
we have a substantially higher number of pixels classified as _fagus sylvatica_ in 
total.  

```{r example_seasonal_plots, echo=F, warning=F, message=F, fig.cap="Exampalary histogramm plot of seasonal parameters for RCC by tree species at 25 cm resolution."}

if (!file.exists("ses_data.rds")){
  seasonal_names = readRDS("../../data/season/season_names.rds")
  seasons = raster::stack(list.files("../../data/season", pattern="25", full.names=TRUE))
  names(seasons) = seasonal_names
  fagus =  rasterize(trees[trees$specID == "BUR",], rgb, 1)
  quercus =  rasterize(trees[trees$specID == "EIT",], rgb, 1)
}

if (!file.exists("ses_data.rds")){
  ses_fagus = as_tibble(seasons[fagus])
  ses_fagus$species = "fagus"
  ses_quercus = as_tibble(seasons[quercus])
  ses_quercus$species = "quercus"
  ses = bind_rows(ses_fagus, ses_quercus)
  paras = c("MAX", "MIN", "AMP", "SD", "SUM", "Q25", "Q75")
  for (i in paras){ names(ses) = str_replace(names(ses),i, paste("-", i, sep =  ""))}
  names(ses) = str_remove(names(ses), "season_")
  ses = ses %>%
    gather(key, value, -species) %>%
    separate(key, c("index", "parameter"), "-")
  saveRDS(ses, "ses_data.rds")
} else {
  ses = readRDS("ses_data.rds")
}

ggplot(filter(ses, index == "GCC"), aes(value, fill=species)) +
  facet_wrap(. ~ parameter, scales = "free")+
  geom_histogram(bins = 80, alpha = 0.6, position = "identity")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  ylab("Number of pixels")+
  xlab("Value of GCC index")
```


## Classification and Validation

For the classification of tree species we used a Random Forest model based
on a forward-feature selection of predictor variables stratified by a leave-location-out 
five-fold cross-validation technique (LLOCV). Random Forest is a non-parametric model, both suitable
for regression and classification problems. It was developed by @Breiman2001 and 
it works by the establishment of a number of decision trees, the forest, each 
constructed on a random split of predictor variables. The final class decision is
made by a majority vote of all trees in the forest. We used the implementation
in the `caret` package [@R-caret] as well as the `CAST` package to implement the LLOCV [@R-CAST]. 

To validate the models, the overall accuracy (OA) is determined. OA describes
the number of correctly classified observations divided by the total number
of observations and is used in a number of studies to determine the validity
of classification models [@Onishi2018a; @Sothe2019a; @Natesan2019].
As a another statistical measure of quality, the accuracy of species 
classification on an object basis is investigated. An object is considered 
correctly classified if a majority of the pixels belonging to the object corresponds 
to the class of this object. Here we consider the 2m x 2m squares around each
GPS point as an representation of the corresponding tree object. Since we did not
used a tree crown delineation, this is an over-simplistic object definition, thus
we refer to it as a pseudo-object classification form here on.

To generate further insights into the importance of single variables and indices,
we evaluated the variable importance.
The variable importance describes the explanatory portion a variable has in a 
given model in percent. The variable 
with the highest explanatory share is set to 100% and the explanation share of 
the remaining variables is shown in relation to this variable with the highest
explanation power, thus indicating percentages below 100%.

# Results


Figure 5 shows that all models, excluding the model at a 25cm resolution including
all variables, regardless of the use of seasonal and vegetation indices and 
resolution, show an OA below 0.65. 
The OA increases for all predictor combinations with an increase in spatial resolution. 
The comparison shows that the models calculated exclusively with 
mono-temporal indices show a substantially higher OA than the models 
calculated exclusively with seasonal indices (~3%). The inclusion of both, mono-
temporal and seasonal parameters, increases the OA at a rate of about 1%.


```{r result_plots,  warning=F, message=F, fig.cap="Overall accuracy of the random forest models based varying pixel sizes and predictor variables (green - mono-temporal indices, blue - seasonal indices, red - mono-temporal and seasonal indices)", out.width="60%"}

models = list.files("../results", pattern = "\\.rds$", full.names = T)

getModelMetric <- function(model, metric){
  decomp = str_split(str_split(model, "/")[[1]][3], "_")[[1]]
  if (length(decomp) == 2){
    res = str_sub(decomp[2],4,5)
    type = "all"
  } else {
    res =  str_sub(decomp[3],4,5)
    type = decomp[2]
  }
  
  model = readRDS(model)
  besthyper = as.numeric(model$bestTune)
  if (!metric %in% c("Kappa", "Accuracy")) stop("chosen metric not valid.")
  if (metric == "Kappa"){
    metricVal = model$results$Kappa[which(model$results$mtry == besthyper)]}
  if (metric == "Accuracy"){
    metricVal = model$results$Accuracy[which(model$results$mtry == besthyper)]}
  df_return = data.frame(resolution=res, type=type, metric=metricVal)
  names(df_return)[3] = metric
  return(df_return)
}

# kappas = do.call("rbind", lapply(models, getModelMetric, metric="Kappa"))
# kappas$type = factor(kappas$type, levels(kappas$type)[c(2,1,3)])
# 
# ggplot(data = kappas, aes(x=resolution)) +
#   geom_point(aes(y=Kappa, shape=type, col=type), size=2)+
#   theme_minimal()


accuracies = do.call("rbind", lapply(models, getModelMetric, metric="Accuracy"))

ggplot(data = accuracies, aes(x=resolution)) +
  geom_point(aes(y=Accuracy, shape=type, col=type), size=3)+
  ylab("Overall accuracy") +
  xlab("Spatial resolution")+
  theme_minimal()

```

Figure 6 shows the percentage of the pseudo-objects classified as 
the correct species based on the majority of pixels within its boundaries. 
It is clearly shown that, regardless of the variables used, 
the accuracy of pseudo-object classification increases from a resolution of 10 cm to a 
resolution of 15 cm. The resolution of 20cm, on contrast, has either a negative effect 
or not effect at all on the accuracy for all models. It is striking that the models 
where mono-temporal and seasonal indices were used in combination had the 
lowest accuracy in object classification but achieved the highest OA 
on a pixels basis (Figure 5).

```{r validation_plots, warning=F, message=F, fig.cap="Overall Accuracy of species classification on an pseudo-object basis.", out.width="60%"}
predictions = readRDS("../../results/validation_pred.rds")
index = predictions$index

metrics = predictions %>%
  select(-index) %>%
  map(function(x) factor(x, levels=1:2, labels=c("BUR", "EIT"))) %>%
  map(function(x) caret::confusionMatrix(trees$specID[index], x)) %>%
  map(function(x) tibble(accuracy = x$overall[1], kappa = x$overall[2] ))

results = do.call("rbind", metrics)
resolutions = stringr::str_sub(names(predictions)[-10],-2,-1)

types = stringr::str_sub(names(predictions)[-10],5, 7)
types[types=="IND"] = "indices"
types[types=="SES"] = "seasons"
types[types=="ALL"] = "all"
results$resolution = factor(resolutions)
results$type = factor(types) 


ggplot(data = results, aes(x=resolution)) +
  geom_point(aes(y=kappa, shape=type, col=type), size=3) +
  ylab("Overall accuracy") +
  xlab("Spatial resolution")+
  theme_minimal()

```


Figures 7, 8 and 9 show the predictors and their 
explanatory portions for the models with mono-temporal indices, seasonal indices 
and both variable types in combination. The explanation percentages of the 
individual predictors were calculated from the respective models. 

For models with mono-temporal predictors, the GCC, GLI and VVI indices are of 
particular high importance (Fig. 7). It is noticeable that the aerial photographs 
of June 5th play an important role in the model calculation. When seasonal 
predictors are used for model calculation, Figure 8 shows that the RCC index 
is of great importance. Also, seasonal parameters of the TGI appear three times.

Looking at the explanatory share for models calculated using a combination of 
mono-temporal and seasonal indices, it is noticeable that significantly more 
mono-temporal predictors are important than seasonal predictors. Furthermore, 
it can be seen that the importance of mono-temporal predictors for model 
explanation changes in comparison to Figure 7. However, it is observed that the 
GCC index of April 29th is of higher importance compared to the mono-temporal 
predictors models only. In addition, completely different predictors, 
irrespective of whether they are mono-temporal or seasonal, 
have an influence on the model explanation than in the models with exclusively 
mono-temporal or seasonal predictors. The importance of predictors for June 5th,
however, remains also in the combination models. 
The situation is similar for the GCC, RCC and VVI indices, which are more frequently
present as high importance explanatory variables than other indices.

```{r var_imp_indices, echo=F, warning=F, message=F, fig.cap="Variable importance for mono-temporal (left) and seasonal (right) predictors averaged accross resolutions.", out.height="40%"}
getVarImp <- function(model){
  decomp = str_split(str_split(model, "/")[[1]][3], "_")[[1]]
  if (length(decomp) == 2){
    res = str_sub(decomp[2],4,5)
    type = "all"
  } else {
    res =  str_sub(decomp[3],4,5)
    type = decomp[2]
  }
  
  model = readRDS(model)
  tmp = importance(model$finalModel)[,3]
  vars = names(tmp)
  
  indexR = which(str_detect(vars, "res5.1"))
  indexG = which(str_detect(vars, "res5.2"))
  indexB = which(str_detect(vars, "res5.3"))
  if( length(indexR) > 0) vars[indexR] = str_replace(vars[indexR], "res5.1", "R")
  if( length(indexG) > 0) vars[indexG] = str_replace(vars[indexG], "res5.2", "G")
  if( length(indexB) > 0) vars[indexB] =str_replace(vars[indexB], "res5.3", "B")
  
  results_df = data.frame(variable=vars, varImp=tmp, resolution=res, type=type)
  rownames(results_df) = NULL
  return(results_df)
}

metric = lapply(models, getVarImp)
metrics = as_tibble(do.call("rbind", metric))
paras = c("MAX", "MIN", "AMP", "SD", "SUM", "Q25", "Q75")
for (i in paras){ metrics$variable = str_replace(metrics$variable,i, paste("-", i, sep =  ""))}
for (i in as.character(indicesdf$abbreviation)) {metrics$variable = str_replace(metrics$variable, paste0("_",i), paste("-", i, sep =  ""))}
metrics$variable = str_replace(metrics$variable, "X2","2")
metrics$variable = str_remove(metrics$variable, "season-")
metrics$variable = str_replace_all(metrics$variable ,"_","/")
metrics$variable = factor(metrics$variable)

varImpInd = filter(metrics, type == "indices")
varImpInd$variable = droplevels(varImpInd$variable)
varImpInd$variable = reorder(varImpInd$variable, varImpInd$varImp)

varImpSes = filter(metrics, type == "season")
varImpSes$variable = droplevels(varImpSes$variable)
varImpSes$variable = reorder(varImpSes$variable, varImpSes$varImp)

varImpAll = filter(metrics, type == "all")
varImpAll$variable = droplevels(varImpAll$variable)
varImpAll$variable = reorder(varImpAll$variable, varImpAll$varImp)

plt1 = varImpInd %>%
  group_by(variable) %>%
  summarise(value = sum(varImp, n = n())) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, value)) %>%
  mutate(value = value / max(value) * 100) %>%
  ggplot(aes(x=variable, y=value))+
  geom_point(color = "dodgerblue1", size = 2)+
  geom_linerange(aes(ymin = 0, ymax = value), color = "dodgerblue1", linetype = 2) +
  theme_minimal()+
  theme(text = element_text(size=6))+
  coord_flip()+
  xlab("Predictor variable")+
  ylab("Relative importance")

plt2 = varImpSes %>%
  group_by(variable) %>%
  summarise(value = sum(varImp, n = n())) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, value)) %>%
  mutate(value = value / max(value) * 100) %>%
  ggplot(aes(x=variable, y=value))+
  geom_point(color = "dodgerblue1", size = 2)+
  geom_linerange(aes(ymin = 0, ymax = value), color = "dodgerblue1", linetype = 2) +
  theme_minimal()+
  theme(text = element_text(size=6))+
  coord_flip()+
  xlab("")+
  ylab("Relative importance")


grid.arrange(plt1, plt2, ncol = 2)
```

```{r var_imp_all, echo=F, warning=F, message=F, fig.cap="Variable importance for mono-temporal and seasonal predictors averaged accross resolutions (left) and averaged variable importance across all model types and resoultions (right).", out.height="40%"}

plt1 = varImpAll %>%
  group_by(variable) %>%
  summarise(value = sum(varImp, n = n())) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, value)) %>%
  mutate(value = value / max(value) * 100) %>%
  ggplot(aes(x=variable, y=value))+
  geom_point(aes(y = value), color = "dodgerblue1", size = 2)+
  geom_linerange(aes(ymin = 0, ymax = value), color = "dodgerblue1", linetype = 2) +
  theme_minimal()+
  theme(text = element_text(size=6))+
  coord_flip()+
  xlab("Predictor variable")+
  ylab("Relative importance")

plt2 = metrics %>%
  group_by(variable) %>%
  summarise(value = sum(varImp, n = n())) %>%
  ungroup() %>%
  mutate(variable = fct_reorder(variable, value)) %>%
  mutate(value = value / max(value) * 100) %>%
  ggplot(aes(x=variable, y=value))+
  geom_point(aes(y = value), color = "dodgerblue1", size = 2)+
  geom_linerange(aes(ymin = 0, ymax = value), color = "dodgerblue1", linetype = 2) +
  theme_minimal()+
  theme(text = element_text(size=6))+
  coord_flip()+
  xlab("")+
  ylab("Relative importance")

grid.arrange(plt1, plt2, ncol = 2)
```

# Discussion & Conclusion

By combining mono-temporal and seasonal parameters we were able to achieve an
OA accuracy of 64.8% at a spatial resolution of 25 cm. It seems that the 
spatial aggregation of RGB information into coarser resolutions can help 
to correctly identify tree species. On a pseudo-object based level,
without incorporating a true crown delineation, we achieved an maximum OA of 97.1 %.
On a pixel basis, we achieve slightly lower OA compared to other studies, conducting similar
research in distinct forest environments of the world [@Onishi2018a; @Sothe2019a; @Fricker2019; @Natesan2019].
@Onishi2018a achieved an OA of 83.1 % by incorporating a tree crown segmentation
and training the GoogLeNet neural-network with six different tree species in 
a forest stand in the warm and humid climate close to Kyoto, Japan.
@Sothe2019a achieved a maximum OA of 72.4 % when next to predictor variables
of the visible and near infrared spectrum other variables from a photogrammetric
point cloud and the canopy height model were included in a support-vector-machine.
They differentiated between 12 tree species in a subtropical forest in Southern Brazil.
@Fricker2019 used an convolutional-neural-network in a mixed-conifer forest in
Northern America and report an F-Score of 64 % for RGB predictors only to differentiate
10 tree species. 
@Natesan2019 trained a residual network on RGB based predictors to classify between
pine trees and non-pine trees in a tree crown segmented image and achieved an OA
of 80 %.
When comparing our results to other studies, we have to note that our study area
is of less complexity, than for example tropical or sub-tropical forests. We only
have two different species in our area and the training data set is slightly 
unbalanced towards the class of _fagus sylvatica_. This can contribute to
an over-classification of this class, leading the trained model to more frequently
assign the _fagus sylvatica_ class to pixels, and by chance increasing the OA. 
Next to applying several iterations of cross-validations to account for this error,
a more balanced training set could prove useful to achieve more stable results.
Concerning the results of the pseudo-object based classification, it is indicated
by this study that object-based approaches can significantly increase the OA.
As reported by other studies, with the use of neural-networks and a thorough
tree crown delineation, very high accuracies can be achieved 
only based on RGB data even in more complex forest environments [@Fricker2019; @Natesan2019]. 

Additionally, sources of error in the imagery can have a significant influence on
the outcome of a classification process. The images used in this study
are prone to a number of sources of errors. Firstly, we observed inconsistencies 
in the process of georeferencing the images which led to rather large image distortions (Fig. 2). 
In dense canopy structures, such as our study area, the generation of tie points
to correctly match objects from different images, is a function
of flight path and height as well as the chosen sensor and can lead to very
high margins of error in the localization of pixels [@Fraser2018a]. Another related issue are the differences 
in illumination and viewing geometry between several overflights, which seriously limit the comparability
of images from different days or even different paths [@Fraser2018a; @Fricker2019; @Sonnentag2012].
Also, the frequency of overflights is quite high for April and May, however, it
is reduced between Mid-May to the end of June, leading to a lower sampling
frequency well within the vegetation period. 
These errors are amplified in the current study by our negligent approach to
the localization of individual trees within and across the image time series (Fig. 1).
The chosen approach was intended to focus solely on the analysis of the influence
of spatial resolution and mono-temporal vs. multi-temporal predictors. However,
this makes our results less comparable to other studies and, in fact, useful for
practitioners of forestry environmental protection. Without a clear approach
to delineate tree crowns and track their development trough a time-series of images,
the results presented here remain of academic interest only.

The results of the influences of different resolutions and
the incorporation of mono- and seasonal-predictors, still have the potential
to highlight valid insights. Models trained with both mono-temporal and seasonal
indices performed best overall closely followed by models trained with mono-temporal 
indices. Thus leading us to the conclusion, that mono-temporal predictors 
are more important than seasonal predictors but a combination of both positively increase the 
model performance. A reason for the rather poor performance of the 
seasonal parameters might be the temporal resolution of available UAV overpasses. 
It is possible that with low-frequency observation flights, the important development processes 
of vegetation growth and plant senescence might not be captured. Concerning the
influence of the spatial resolution, there is an indication that averaging
high resolution pixels towards larger spatial units can improve the classification.
The results, however, remain inconclusive when the pixel-based approach is compared
to the object-based, since in the latter we observe a decrease in OA above a 
certain threshold in resolution.

It is thus highly recommended to further analyse the impact of object-based 
classification strategies on the achievable accuracy. The potential benefit
of using low-cost UAVs and sensors for tree species classification is evident,
but there remain open question towards which is the most sensible approach
to bring this data to efficient uses. To achieve this, a robust scheme of crown delineation 
seems to be of high importance in order to get a less erroneous training data set.
Under these conditions, we suspect the gap between the accuracies of pixel-based and
object-based classification methods to be smaller, however, the dynamic of the 
classification accuracy across changing spatial resolutions seems to be worth to 
be further investigated.

# References


