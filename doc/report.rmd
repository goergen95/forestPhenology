---
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
    latex_engine: pdflatex
    fig_caption: true
bibliography: ForestPhenology.bib
csl: elsevier-harvard.csl
title: "UAV imagery based tree species classification in the Marburg OpenForest"
author: 
- Darius A. Görgen
- Tobias Koch
- Marvin Müsgen
- Eike Schott
date: "`r format(Sys.time(), '%B %d, %Y')`"

abstract: "The monitoring of forests environments is of crucial importance since they serve as natural habitats and constitute a main source of biological diversity on the planet. Yet, it is very costly and labor intensive to monitor forests by traditional means and classical remote sensing technologies restrict the analysis to the regional level. To overcome these challenges there have been several attempts to use UAV-borne imagery in forest monitoring. By the use of drones images can be obtained at low cost and can be associated with both, high spatial and temporal resolution. This enables scientists and practitioners to comprehensively monitor forest environments. Tree species identification is primary interest, since the identification of species allows to draw conclusions about the structure and biodiversity in given areas of a forest. When using simple RGB images, species classification still remains a challenge. In this paper we present our results of an experiment exploring the influence of spatial resolution of RGB imagery, artificially derived vegetation indices as well as seasonal parameters on the accuracy of tree species classification within the Marburg OpenForest. We used a resolution of 5 cm, 10 cm, 15 cm, and 25 cm in a forward-feature-selection based on the Random Forest classification algorithm. Additionally we tested the obtained accuracy when only mono-temporal or multi-temporal variables are included as well as both types of variables. Our results show that ..."

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tmap)
library(caret)
library(raster)
library(stringr)
library(ggplot2)
```

# Introduction

Forests environments provide valuable services to the human well being as well as 
supporting services to the function of ecosystems. Additionally, they count to 
the main biodiversity hotspots on earth [@Brockerhoff2017]. However, the most recent
Global Forest Resource Assessment of the FAO states, that the global forest area
shrinked between 1990 and 2015 from 31.6% of the global land cover to 30.6%, or in 
other numbers from 4,128 Mio. ha to 3,999 Mio. ha - a decrease of 3.1% [@FoodandAgricultureOrgansiationoftheUnitedNations2015].
Simultaneously, the percentage of planted trees increased by over 105 Mio. ha, 
reducing the share of natural forest areas. Over the last decades, we can find growing
scholar interests on ecosystem services and functions which are sustained by forests,
mainly the habitat provision for endangered or native species, the provision of 
material goods such as wood biomass, soil formation and composition, as well as 
climatic regulation functions, such as carbon sequestration [@Brockerhoff2017].
One of the main critical variables in assessing the quality of forest environments,
their biodiversity, and their structural attributes is the tree species, either on
the individual tree level or the dominating species for coarser areas of interest. 

Identifying tree species, however, on an operative scale for larger areas, remains a challenge.
Recently, different remote sensing approaches proved that tree species identification
from above the earth's surface is feasible, but there still remain significant 
trade-offs between the costs, computational demand and spatial-temporal resolution of
remotely sensed imagery. Some studies have used satellite imagery which most frequently also
include information in the infrared spectrum [@Zhang2003; @Elatawneh2013; @Ulsig2017a; @Persson2018; @Grabska2019]
but generally shows a relatively low spatial resolution limiting the analysis to a 
level of stand rather than individuals. Additionally, depended on the platform's orbit
and current weather conditions, the temporal resolution may vary significantly and is 
not completely planable.

One technology to partly overcome these restrictions is the use of Radio Detection And
Ranging (RADAR) sensors which show a lower dependency of data quality to the presence
of clouds and fog. Recently, multi-temporal data from Sentinel-1 has been used in 
conjunction with spectral data to not only improve the species classification accuracy but
also to retrieve additional parameters important to forest monitoring such as forest type, 
stand density, annual phenology, and biomass production among others [@Frison2018; @Niculescu2018; @Dostalova2018; @Mngadi2019].

However promising these advances, the analysis are most commonly restricted to regional analysis
of forest structures. On a more localized level, high resolution satellite data is 
either not available or associated with very high costs. With the rapid development of
unmanned aerial vehicles (UAV) during the last years and a significant decrease in price for
this technology, new approaches to monitor forest structures on a very local level
have recently emerged [@Yao2019b].  

UAVs serve as an aerial platform of different kind of sensors which can range from
LidAR [@Fricker2019], hyper- and multi spectral sensors [@Marques2019a; @Berra2019] as well as simple RGB cameras [@Natesan2019]. A broad methodology to get species information for 
single tree individuals has been developed integrating the calculation of various 
vegetation indices from mono- and multi-temporal imagery and the use of machine learning
models to derieve a relationship between the measured variables and the tree species [@Berra2016; @Klosterman2017; @Klosterman2018; @Sadeghi2018; @Natesan2019; @Fricker2019; @Berra2019].
For example, @Klosterman2017 were able to estimate phenological status on the leaf-level
based on the calculation of the green and red chromatic coordinate (GCC and RCC) 
during spring and autumn.  

Additionally, the development of structure-from-motion algorithms 
to get 3D information from 2D RGB imagery taken from slightly different angles have enriched the analysis of forest structures from low-cost sensors [@Nevalainen2017; @Yan2018a; @Onishi2018a; @Krause2019a; @Brieger2019a; @Sothe2019a]. @Krause2019a were able to retrive individual 
tree height based on a photogrammetric point-cloud with an RMSE at about 2-3 %. 
@Natesan2019 used a Residual Neural Network to effectivly delinate individual tree crowns
from RGB derived point-clouds and used the spectral informaton for a species classification
achieving an overall accuracy of 80%. 

In this study, we evaluate the impact on classification accuracy in terms of 
Kappa scores based on spatial resolution and a comparison between mono-temporal
and seasonal artificial indices. 


# Data and Method

In this project, AgisoftPhotoScan (???) in the version (???) was used to process the UAV imagery. Agisoft Photoscan Professional is anaffordable 3D reconstruction software from the Russian company Agisoft LLC (Agisoft, 2018) for the generation of dense point clouds and photogrammetric products such as orthorectified mosaics and DSM derived from images. Photoscan has the advantage to provide a simple workflow, from performing bundle block adjustment to calibrate the camera and orientate images after automatic tie point measurements, geo-referencing by measuring ground control points, concluding with the computation of a dense point cloud and requested final products (Mayer et al. 2018 ???? Mendely hinzuf?gen). The Airborne system (???Drohnenname) was used to acquire the UAV imagery using a commercial goPro (???Model + Objektiv. In (??? Anzahl flights) flight campaigns with a flight altitude of (40 meters???), (???Anzahlfotos) photos were acquired. The internal GPS of the GoPro was used for geotagging the images. A post referencing enabled a better processing of the images in SFM software (Version???) and more accurate orthophotos without manual referencing in Photoscan.  Photo Alignment is a process in PS for image matching and bundle block adjustment in an arbitrary system. It generates a sparse point cloud as well as the interior and exterior orientation parameters of all images in that system, including systematic error compensation such as non-linear lens distortions. Prior to the adjustment, the tie points are automatically measured by detecting and matching features in overlapping images resulting in a sparse point cloud (Mayer et al. 2018??? Mendely hinzuf?gen).The settings in this project were chosen as follows:
- General: Accuracy: Medium; Generec preselection: yes; Reference preselection: yes
- Advances: Key Piont limit: 40000; Tie point limit: 4000; apply mask: no; Adaptive camera model fitting: yes

Sparse cloud filtering was performed under the following settings:
-gradual Selection: reprojection error: 0.26; reconstruction uncertainty: 189.461; projection accuracy:12.4621; reconstruction uncertainty:6.72951; reprojectionerror:0.122199

Based on the information of the point cloud (Sparse Cloud, Dense Cloudetc  ...) PhotoScan can construct a polygon model (Mesh) (Agisoft 2018??? Mendeley hinzuf?gen) In this Project the Mesh was build by following setting:
- General: surface type: Height field (2.5D); source data: sparse cloud; face count: medium
- advanced: Interpolation: enabled; Point classes: all; calculate vertex colors

The different pixel values from different photos are combined by the Mosaic type in the final texture. Mosiac type implies a two-step approach. Low frequency components are blended for overlapping images to avoid seam line problems. The high-frequency component, on the other hand, which is responsible for the image details, is only captured from a single image.


In total we worked on 6 images which were obtained between the end of April to 
the end of June. In the mitlatitudes of central Europe these are the months of vegetational
peak of mixed forests. We calculated a selection of RGB-based vegetational indices for each
of the images as well as seasonal statistics which describe the development of these indices
in the course of the vegetation period.

- UAV orthoimages, AgiSoft (how-to), flight height
- pre-process: crop to AOI, spatial aggregations different resolutions, indices, season parameters
- tree shape: differential GPS points, >= 40cm DBH, 2m buffer, squares -> species
- RF model, 5-fold cross validation, training vs. testing, kappa & accuracy,
- plots predictors (all, indices, seasons) vs. resolutions 


```{r dates-table, echo=F, tab.cap="Days of UAV overpass.",}
dates = unique(readRDS("../../data/resampled/dates.rds"))
dates = as.Date(dates, format = "%Y_%m_%d" )
dates = data.frame(Dates=dates)
knitr::kable(dates, align="l", caption = "Dates of the UAV overpasses.",)

```

```{r cv_visualization, warning=F, message=F, echo=F, fig.cap="Overview of the training and validation trees of the five-fold cross-validation."}
trees = rgdal::readOGR("../../data/trees_buffer.shp")
trees@data$polID = seq(nrow(trees@data))
set.seed(1899)
index = caret::createDataPartition(y = trees@data$polID, p = .70, list = FALSE)
pred = trees@data[-index, ]
ind = CAST::CreateSpacetimeFolds(pred, spacevar = "polID", k = 5)

trees@data$train[index] = 0
trees@data$train[-index] = 1

trees@data$cv1[trees$id %in%pred$id[ind$index[[1]]]] = 1
trees@data$cv1[trees$id %in%pred$id[ind$indexOut[[1]]]] = 0

trees@data$cv2[trees$id %in%pred$id[ind$index[[2]]]] = 1
trees@data$cv2[trees$id %in%pred$id[ind$indexOut[[2]]]] = 0

trees@data$cv3[trees$id %in%pred$id[ind$index[[3]]]] = 1
trees@data$cv3[trees$id %in%pred$id[ind$indexOut[[3]]]] = 0

trees@data$cv4[trees$id %in%pred$id[ind$index[[4]]]] = 1
trees@data$cv4[trees$id %in%pred$id[ind$indexOut[[4]]]] = 0

trees@data$cv5[trees$id %in%pred$id[ind$index[[5]]]] = 1
trees@data$cv5[trees$id %in%pred$id[ind$indexOut[[5]]]] = 0

rgbimage = raster::stack("../../data/resampled/res25.tif")
rgb_names = readRDS("../../data/resampled/names_RGB_stack.rds")
names(rgbimage) = rgb_names 

rgbimage = rgbimage[[13:15]]


# tm_shape(rgbimage)+
#   tm_rgb()+
#   tm_shape(trees)+
#   tm_fill(col = "train", palette = "RdBu")


tm_shape(rgbimage)+
  tm_rgb()+
  tm_shape(trees, col = c("red","blue","grey"))+
  tm_polygons(c("cv1","cv2","cv3","cv4","cv5"))

```

# Results


```{r result_plots, echo=F, warning=F, message=F, fig.cap="Kappa scores of the random fores models based varying pixel sizes and predictor variables (red - mono-temporal indices, blue - seasonal indices, green - mono-temporal and seasonal indices)"}

models = list.files("../results", pattern = "\\.rds$", full.names = T)

getModelMetric <- function(model, metric){
    decomp = str_split(str_split(model, "/")[[1]][3], "_")[[1]]
    if (length(decomp) == 2){
      res = str_sub(decomp[2],4,5)
      type = "all"
    } else {
      res =  str_sub(decomp[3],4,5)
      type = decomp[2]
    }
    
    model = readRDS(model)
    besthyper = as.numeric(model$bestTune)
    if (!metric %in% c("Kappa", "Accuracy")) stop("chosen metric not valid.")
    if (metric == "Kappa"){
      metricVal = model$results$Kappa[which(model$results$mtry == besthyper)]}
    if (metric == "Accuracy"){
      metricVal = model$results$Accuracy[which(model$results$mtry == besthyper)]}
    df_return = data.frame(resolution=res, type=type, metric=metricVal)
    names(df_return)[3] = metric
   return(df_return)
  }
  
kappas = do.call("rbind", lapply(models, getModelMetric, metric="Kappa"))

ggplot(data = kappas, aes(x=resolution)) +
  geom_point(aes(y=Kappa, shape=type, col=type), size=2)+
  theme_minimal()


#accuracies = do.call("rbind", lapply(models, getModelMetric, metric="Accuracy"))

# ggplot(data = accuracies, aes(x=resolution)) +
#   geom_point(aes(y=Accuracy, shape=type, col=type), size=2)+
#   theme_minimal()

```

```{r object_accuracy, echo=F, warning=F, message=F, eval=F}
models = readRDS(list.files("../results", pattern = "\\.rds$", full.names = T))

```

# References



