---
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    df_print: kable
    latex_engine: pdflatex
    fig_caption: true
header-includes:
  \usepackage{float}
bibliography: ForestPhenology.bib
csl: elsevier-harvard.csl
title: "UAV imagery based tree species classification in the Marburg OpenForest"
author: 
- Darius A. Görgen
- Tobias Koch
- Marvin Müsgen
- Eike Schott
date: "`r format(Sys.time(), '%B %d, %Y')`"

abstract: "The monitoring of forests environments is of crucial importance since they serve as natural habitats and constitute a main source of biological diversity on the planet. Yet, it is very costly and labor intensive to monitor forests by traditional means and classical remote sensing technologies restrict the analysis to the regional level. To overcome these challenges there have been several attempts to use UAV-borne imagery in forest monitoring. By the use of drones images can be obtained at low cost and can be associated with both, high spatial and temporal resolution. This enables scientists and practitioners to comprehensively monitor forest environments. Tree species identification is primary interest, since the identification of species allows to draw conclusions about the structure and biodiversity in given areas of a forest. When using simple RGB images, species classification still remains a challenge. In this paper we present our results of an experiment exploring the influence of spatial resolution of RGB imagery, artificially derived vegetation indices as well as seasonal parameters on the accuracy of tree species classification within the Marburg OpenForest. We used a resolution of 5 cm, 10 cm, 15 cm, and 25 cm in a forward-feature-selection based on the Random Forest classification algorithm. Additionally we tested the obtained accuracy when only mono-temporal or multi-temporal variables are included as well as both types of variables. Our results show that ..."

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "H", fig.align="center")
library(tmap)
library(caret)
library(raster)
library(stringr)
library(ggplot2)
library(dplyr)
library(purrr)
library(rgdal)
trees = readOGR("../../data/trees_buffer.shp")
```

# Introduction

Forests environments provide valuable services to the human well being as well as 
supporting services to the function of ecosystems. Additionally, they count to 
the main biodiversity hotspots on earth [@Brockerhoff2017]. However, the most recent
Global Forest Resource Assessment of the FAO states, that the global forest area
shrinked between 1990 and 2015 from 31.6% of the global land cover to 30.6%, or in 
other numbers from 4,128 Mio. ha to 3,999 Mio. ha - a decrease of 3.1% [@FoodandAgricultureOrgansiationoftheUnitedNations2015].
Simultaneously, the percentage of planted trees increased by over 105 Mio. ha, 
reducing the share of natural forest areas. Over the last decades, we can find growing
scholar interests on ecosystem services and functions which are sustained by forests,
mainly the habitat provision for endangered or native species, the provision of 
material goods such as wood biomass, soil formation and composition, as well as 
climatic regulation functions, such as carbon sequestration [@Brockerhoff2017].
One of the main critical variables in assessing the quality of forest environments,
their biodiversity, and their structural attributes is the tree species, either on
the individual tree level or the dominating species for coarser areas of interest. 

Identifying tree species, however, on an operative scale for larger areas, remains a challenge.
Recently, different remote sensing approaches proved that tree species identification
from above the earth's surface is feasible, but there still remain significant 
trade-offs between the costs, computational demand and spatial-temporal resolution of
remotely sensed imagery. Some studies have used satellite imagery which most frequently also
include information in the infrared spectrum [@Zhang2003; @Elatawneh2013; @Ulsig2017a; @Persson2018; @Grabska2019]
but generally shows a relatively low spatial resolution limiting the analysis to a 
level of stand rather than individuals. Additionally, depended on the platform's orbit
and current weather conditions, the temporal resolution may vary significantly and is 
not completely planable.

One technology to partly overcome these restrictions is the use of Radio Detection And
Ranging (RADAR) sensors which show a lower dependency of data quality to the presence
of clouds and fog. Recently, multi-temporal data from Sentinel-1 has been used in 
conjunction with spectral data to not only improve the species classification accuracy but
also to retrieve additional parameters important to forest monitoring such as forest type, 
stand density, annual phenology, and biomass production among others [@Frison2018; @Niculescu2018; @Dostalova2018; @Mngadi2019].
However promising these advances, the analysis are most commonly restricted to regional analysis
of forest structures. On a more localized level, high resolution satellite data is 
either not available or associated with very high costs. With the rapid development of
unmanned aerial vehicles (UAV) during the last years and a significant decrease in price for
this technology, new approaches to monitor forest structures on a very local level
have recently emerged [@Yao2019b].  

UAVs serve as an aerial platform of different kind of sensors which can range from
LidAR [@Fricker2019], hyper- and multi spectral sensors [@Marques2019a; @Berra2019] 
as well as simple RGB cameras [@Natesan2019]. A broad methodology to obtain species information for 
single tree individuals has been developed integrating the calculation of various 
vegetation indices from mono- and multi-temporal imagery and the use of machine learning
models to derieve a relationship between the measured variables and the 
tree species.
@Berra2016 used the Green Chromatic Coordinate to monitor the Start-of-Season for
four different tree species in decidous woodland suggesting that UAV imagery 
can contribute to investiage the phenologocial statu of individuak trees.
@Klosterman2017 were able to estimate phenological status on the leaf-level
based on the calculation of the green and red chromatic coordinate (GCC and RCC) 
during spring and autumn to budburst and leaf expansion as well as leaf senescence.
@Natesan2019 used Residual Neural Networks to classify three different tree species based on RGB
imagery obtained over the course of three years and achived a classification accuracy
of about 80%, and an accuracy of 51% when only the data of single years was used. 
@Fricker2019 used hyperspectral images obtained by a UAV and a Convolutional Neural
Network to classify tree species. They also underwent an experiment which only included RGB
data. The hyperspectral data achieved an F-score of 0.87, while the RGB data achieved 
a score of 0.64 in a dominantly coniferous forest.

Additionally, the development of structure-from-motion algorithms 
to get 3D information from 2D RGB imagery taken from slightly different angles 
have enriched the analysis of forest structures from low-cost sensors. 
@Nevalainen2017 used RGB images and an automated matching technique to obtain 
point clouds at a 5cm resoultion. Coupled with hyperspectral imagery this allowed 
the tree species classification to an accuracy at 95%.
@Yan2018a compared thier approach of retrieving tree crowns from RGB images with
crowns delinated from LiDAR data and report an accuracy at about 90%. @Krause2019a 
were able to retrive individual tree height based on a photogrammetric point-cloud 
with an RMSE at about 2-3 %. @Brieger2019a used RGB derived point clouds at
different study sites in Siberia and achived an accuracy of 67.1% in delinating 
individual tree crowns and an RMSE of 18.46% for tree height. @Sothe2019a used
hyperspectral images for tree delination and classification in subtropical rainforests
and achived and Kappa score of 0.7 (overall accuracy of 72.4%) by combining spectral
raw data, indices as well as structural parameters in the classification process using 
a support-vector machine. 

However, little efforts have been done to structurally investigate the impact of decreasing
spatial resolution on the classification accuracy as well as the impact of
the integration of seasonal parameters derived from multiple mono-temporal observations.
Here, we strictly limit our analysis to the investigation of these two thematic 
blocs and deliberatly exclude other factors such as structural variables obtained
from point clouds. We solely focus on the analysis of the dynamic in predictive potential 
of RGB derived mono- and multi-temporal variables to model tree species with 
changing spatial resoulution. 

To this end we use the RGB imagery obtained by multiple overflights during the year 
2019 from a study side located within the Marburg Univeristy forest which is part
of the research project [Natur 4.0](https://www.uni-marburg.de/en/fb19/natur40/). 
This forest is used as a joint research area for a project between several German universities 
and research institutes and sets out to investigate the potential of sensor technology for 
biodiversity and natural resource management in natural envrionments forests. 
We artificially decreased the spatial resoultion of aerial imagery obtained in this
area resulting in three different target resolutions of 10, 15, and 25 cm. 
For every single overflight we calculated a series of RGB indices on a pixels basis. 
Additionally, we calculate descriptive statistics (mean, maximum, minimum, amplitude, etc.) 
for each index over the course of the year to include information about the 
seasonality of the phenological development.

The resulting input data is used to establish a total of nine distinct random forest models, one
for each resolution including either only mono-temporal or seasonal predictor variables 
or both types of variables. On the basis of a forward-feature selection
the variables which carry the most relevant information content for the tree species
classification are then selected and used in the species prediction. The evaluation
of the classification accuracies is compared on a pixel and object basis to draw
conclusions on the importance of spatial resolution as well as the importance
of mono- vs. multi-temporal input data.


# Data and Methods

## Study Area

The study area is located at 50.8°N 8.7°E in the German low mountain range in Hesse.
It is part of the University Forest Caldern where the recently initiated joint research
project Natur 4.0 aims at investigating the use of networked sensor technology for 
biodversity and ecosystem managment and protection. It is in this context, that the aerial images 
which we used here were obtained in an observation campaign during 2019 (see Tab. 1).
The area is approximatley 37,500 m² and continously covered by trees. In this
specific location, only two distinct species are present with stem diameters
at breast height (DBH) greater than 40 cm, namely _fagus sylvatica_ and
_quercus robur_.

```{r dates-table, tab.cap="Days of UAV overpass.",}
dates = unique(readRDS("../../data/resampled/dates.rds"))
dates = as.Date(dates, format = "%Y_%m_%d" )
dates = data.frame(Dates=dates)
knitr::kable(dates, align="l", caption = "Dates of the UAV overpasses.",)
```


## Pre-Processing of the UAV orthoimages

```{r plot_aoi, warning=F, message=F,fig.cap="RGB image of the study area from May 16th 2019 with central positions of tree stems superimposed (Coordinates are presented in UTM32N).", out.width="70%"}

rgb = brick("../../data/resampled/res25.tif")[[10:12]]
projection = "+proj=utm +zone=32 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"

tm_shape(rgb)+
  tm_rgb()+
  tm_shape(trees)+
  tm_borders(lwd=1, lty="solid", col="black")+
  tm_graticules(ticks=T, projection=projection, lines=F, 
                labels.format=list(digits=0),
                n.y=6, n.x=4)+
  tm_layout(inner.margins = 0)
```

In this project, AgisoftPhotoScan (???) in the version (???) was used to process the UAV imagery. Agisoft Photoscan Professional is anaffordable 3D reconstruction software from the Russian company Agisoft LLC (Agisoft, 2018) for the generation of dense point clouds and photogrammetric products such as orthorectified mosaics and DSM derived from images. Photoscan has the advantage to provide a simple workflow, from performing bundle block adjustment to calibrate the camera and orientate images after automatic tie point measurements, geo-referencing by measuring ground control points, concluding with the computation of a dense point cloud and requested final products (Mayer et al. 2018 ???? Mendely hinzuf?gen). The Airborne system (???Drohnenname) was used to acquire the UAV imagery using a commercial goPro (???Model + Objektiv. In (??? Anzahl flights) flight campaigns with a flight altitude of (40 meters???), (???Anzahlfotos) photos were acquired. The internal GPS of the GoPro was used for geotagging the images. A post referencing enabled a better processing of the images in SFM software (Version???) and more accurate orthophotos without manual referencing in Photoscan.  Photo Alignment is a process in PS for image matching and bundle block adjustment in an arbitrary system. It generates a sparse point cloud as well as the interior and exterior orientation parameters of all images in that system, including systematic error compensation such as non-linear lens distortions. Prior to the adjustment, the tie points are automatically measured by detecting and matching features in overlapping images resulting in a sparse point cloud (Mayer et al. 2018??? Mendely hinzuf?gen).The settings in this project were chosen as follows:
- General: Accuracy: Medium; Generec preselection: yes; Reference preselection: yes
- Advances: Key Piont limit: 40000; Tie point limit: 4000; apply mask: no; Adaptive camera model fitting: yes

Sparse cloud filtering was performed under the following settings:
-gradual Selection: reprojection error: 0.26; reconstruction uncertainty: 189.461; projection accuracy:12.4621; reconstruction uncertainty:6.72951; reprojectionerror:0.122199

Based on the information of the point cloud (Sparse Cloud, Dense Cloudetc  ...) PhotoScan can construct a polygon model (Mesh) (Agisoft 2018??? Mendeley hinzuf?gen) In this Project the Mesh was build by following setting:
- General: surface type: Height field (2.5D); source data: sparse cloud; face count: medium
- advanced: Interpolation: enabled; Point classes: all; calculate vertex colors

The different pixel values from different photos are combined by the Mosaic type in the final texture. Mosiac type implies a two-step approach. Low frequency components are blended for overlapping images to avoid seam line problems. The high-frequency component, on the other hand, which is responsible for the image details, is only captured from a single image.


In total we worked on 6 images which were obtained between the end of April to 
the end of June. In the mitlatitudes of central Europe these are the months of vegetational
peak of mixed forests. We calculated a selection of RGB-based vegetational indices for each
of the images as well as seasonal statistics which describe the development of these indices
in the course of the vegetation period.


- UAV orthoimages, AgiSoft (how-to), flight height
- pre-process: crop to AOI, spatial aggregations different resolutions, indices, season parameters
- tree shape: differential GPS points, >= 40cm DBH, 2m buffer, squares -> species
- RF model, 5-fold cross validation, training vs. testing, kappa & accuracy,
- plots predictors (all, indices, seasons) vs. resolutions 

## Auxiliary data

With the use of a diferential GPS the position of tree stems within the study area
was logged during a field campaign. Associated with the positional data, the
tree species as well as the DBH was gathered. As stated before, here we only focused
on the determination of the impact of changing resolutions and multi-temporal
predictor variables on the classification accuracy. Therefore, we simplified the
delination of tree crowns corresponding to the needs of the investigation. First,
we excluded all trees with a DBH below 40 cm, because we assumed that the crowns
of greater trees would cover the smaller ones and thus they could not be observed
on aerial images. Secondly, we buffered the central positions of the residual trees
by a square of 2 x 2 m, assuming that with this size we would essentially cover 
substantial proportions of the associated tree crown. However, some
of these buffered polygons intersected. In these cases, we decided to exclude both
intersecting polygons since any decision to keep one over the other would be arbitrary.
In the end, we obtained 161 tree individuals of which 92 (57%) were _fagus sylvatica_ 
and 69 (43%) were _quercus robur_.


## Classification and Validation

# Results

```{r result_plots,  warning=F, message=F, fig.cap="Kappa scores of the random fores models based varying pixel sizes and predictor variables (red - mono-temporal indices, blue - seasonal indices, green - mono-temporal and seasonal indices)", out.width="60%"}

models = list.files("../results", pattern = "\\.rds$", full.names = T)

getModelMetric <- function(model, metric){
    decomp = str_split(str_split(model, "/")[[1]][3], "_")[[1]]
    if (length(decomp) == 2){
      res = str_sub(decomp[2],4,5)
      type = "all"
    } else {
      res =  str_sub(decomp[3],4,5)
      type = decomp[2]
    }
    
    model = readRDS(model)
    besthyper = as.numeric(model$bestTune)
    if (!metric %in% c("Kappa", "Accuracy")) stop("chosen metric not valid.")
    if (metric == "Kappa"){
      metricVal = model$results$Kappa[which(model$results$mtry == besthyper)]}
    if (metric == "Accuracy"){
      metricVal = model$results$Accuracy[which(model$results$mtry == besthyper)]}
    df_return = data.frame(resolution=res, type=type, metric=metricVal)
    names(df_return)[3] = metric
   return(df_return)
  }
  
kappas = do.call("rbind", lapply(models, getModelMetric, metric="Kappa"))

ggplot(data = kappas, aes(x=resolution)) +
  geom_point(aes(y=Kappa, shape=type, col=type), size=2)+
  theme_minimal()


#accuracies = do.call("rbind", lapply(models, getModelMetric, metric="Accuracy"))

# ggplot(data = accuracies, aes(x=resolution)) +
#   geom_point(aes(y=Accuracy, shape=type, col=type), size=2)+
#   theme_minimal()

```

```{r validation_plots, warning=F, message=F, fig.cap="Overall Accuracy of species classification on an object basis.", out.width="60%"}
predictions = readRDS("../../results/validation_pred.rds")
index = predictions$index

metrics = predictions %>%
  select(-index) %>%
  map(function(x) factor(x, levels=1:2, labels=c("BUR", "EIT"))) %>%
  map(function(x) caret::confusionMatrix(trees$specID[index], x)) %>%
  map(function(x) tibble(accuracy = x$overall[1], kappa = x$overall[2] ))

results = do.call("rbind", metrics)
resolutions = stringr::str_sub(names(predictions)[-10],-2,-1)
types = stringr::str_sub(names(predictions)[-10],5, 7)
results$resolution = as.numeric(resolutions)
results$type = types

ggplot(data = results, aes(x=resolution)) +
  geom_point(aes(y=accuracy, shape=type, col=type), size=2)+
  theme_minimal()

```

# Discussion
# Conclusion
# References



